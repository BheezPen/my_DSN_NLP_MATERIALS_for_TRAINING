{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BheezPen/DSN_NLP_MATERIAL_TRAINING/blob/main/COLABipynb_files/02_Tokenization_in_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d939bdb0-74d7-40cf-b515-307adbda6b11",
      "metadata": {
        "id": "d939bdb0-74d7-40cf-b515-307adbda6b11"
      },
      "source": [
        "# üìå  Tokenization in NLP - Beginner to Advanced Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcf868ca-82de-4f7b-884b-e9c53449b79d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "fcf868ca-82de-4f7b-884b-e9c53449b79d",
        "outputId": "d4077b57-4fd2-4cf2-ae61-6a130b237ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTokenization is the process of breaking text into meaningful units (tokens).\\nThere are different types of tokenization:\\n  - Word Tokenization: Splitting text into words.\\n  - Sentence Tokenization: Splitting text into sentences.\\n  - Subword Tokenization: Used in deep learning (BPE, WordPiece).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "import re\n",
        "\n",
        "# Ensure required datasets are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the 'punkt_tab' resource\n",
        "\n",
        "### 1Ô∏è‚É£ Introduction to Tokenization\n",
        "\"\"\"\n",
        "Tokenization is the process of breaking text into meaningful units (tokens).\n",
        "There are different types of tokenization:\n",
        "  - Word Tokenization: Splitting text into words.\n",
        "  - Sentence Tokenization: Splitting text into sentences.\n",
        "  - Subword Tokenization: Used in deep learning (BPE, WordPiece).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Download sentence tokenizer data\n",
        "#nltk.download('punkt_tab') # Download the 'punkt_tab' resource\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"The cat sat on the mat. The dog barked.\"\n",
        "\n",
        "word = word_tokenize(text)\n",
        "\n",
        "print(\"Word:\", word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMEhDeK6bT81",
        "outputId": "69f25df3-f250-42eb-baf7-729b42706b5f"
      },
      "id": "iMEhDeK6bT81",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.', 'The', 'dog', 'barked', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text for tokenization\n",
        "txt = \"Hello everyone! Welcome to the world of NLP. Today, we'll learn tokenization.\"\n",
        "\n",
        "### 2Ô∏è‚É£ Word Tokenization\n",
        "word_tokens = word_tokenize(txt)\n",
        "print(\"Word Tokens:\", word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kms9DxcNfyYR",
        "outputId": "d092990a-623e-4948-9661-6f2888f09eaf"
      },
      "id": "Kms9DxcNfyYR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Hello', 'everyone', '!', 'Welcome', 'to', 'the', 'world', 'of', 'NLP', '.', 'Today', ',', 'we', \"'ll\", 'learn', 'tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98fb511d-c98d-49a4-ab0c-2dfff7886afd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98fb511d-c98d-49a4-ab0c-2dfff7886afd",
        "outputId": "b7a66898-11df-42be-ee9f-7cd9e2abd6ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: ['Hello everyone!', 'Welcome to the world of NLP.', \"Today, we'll learn tokenization.\"]\n"
          ]
        }
      ],
      "source": [
        "### 3Ô∏è‚É£ Sentence Tokenization\n",
        "sentence_tokens = sent_tokenize(txt)\n",
        "print(\"Sentence Tokens:\", sentence_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 4Ô∏è‚É£ Regex-Based Tokenization\n",
        "# Custom tokenization using regular expressions\n",
        "regex_tokens = re.findall(r'\\b\\w+\\b', txt)\n",
        "print(\"Regex Tokens:\", regex_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz1Mu8rzglG4",
        "outputId": "4e1a1cf6-e6b5-4dcf-e987-326e9245c810"
      },
      "id": "Tz1Mu8rzglG4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regex Tokens: ['Hello', 'everyone', 'Welcome', 'to', 'the', 'world', 'of', 'NLP', 'Today', 'we', 'll', 'learn', 'tokenization']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 5Ô∏è‚É£ Subword Tokenization (Byte Pair Encoding - BPE)\n",
        "\n",
        "\n",
        "\n",
        "# Subword Tokenization (Byte Pair Encoding - BPE)\n",
        "# Example using subword tokenization with Hugging Face tokenizers\n",
        "# Using a transformer-based tokenizer\n",
        "import nltk\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # You can use any other model\n",
        "text = \"I have a new unbreakable toy.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnZei_JOgnoG",
        "outputId": "aeb7c6c1-dc5b-4b54-86fd-e39e24b9385e"
      },
      "id": "fnZei_JOgnoG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'have', 'a', 'new', 'un', '##break', '##able', 'toy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Summary:\n",
        "‚úîÔ∏è Word Tokenization - Breaks text into words\n",
        "‚úîÔ∏è Sentence Tokenization - Breaks text into sentences\n",
        "‚úîÔ∏è Regex Tokenization - Custom splitting with regex\n",
        "‚úîÔ∏è Subword Tokenization - Used in deep learning models like BERT\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "DprUl-MdgpVg"
      },
      "id": "DprUl-MdgpVg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}