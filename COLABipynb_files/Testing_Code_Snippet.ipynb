{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEpiCFYRAJzYG4pZ67Gg+5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDwRAvklg-tZ",
        "outputId": "bbc88150-7586-4980-e35d-c47cccbadec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'have', 'a', 'new', 'un', '##break', '##able', 'toy', '.']\n"
          ]
        }
      ],
      "source": [
        "# Subword Tokenization (Byte Pair Encoding - BPE)\n",
        "# Example using subword tokenization with Hugging Face tokenizers\n",
        "# Using a transformer-based tokenizer\n",
        "\n",
        "#from transformers import AutoTokenizer\n",
        "\n",
        "# or\n",
        "import transformers\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\") #Loads the tokenizer for BERT, You can use any other model\n",
        "text = \"I have a new unbreakable toy.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: from transformers import AutoTokenizer, EXPLAIN THAT CODE including where trasnformers is and its library or module name\n",
        "\n",
        "# The line `from transformers import AutoTokenizer` imports the `AutoTokenizer` class from the `transformers` library.\n",
        "\n",
        "# The `transformers` library is a popular Python library developed by Hugging Face.\n",
        "# You can install locally with pip install transformers\n",
        "# It provides pre-trained models and tokenizers for various Natural Language Processing (NLP) tasks.\n",
        "\n",
        "# The `AutoTokenizer` class is a convenient way to load tokenizers associated with pre-trained models.\n",
        "# It automatically determines the correct tokenizer architecture based on the specified model name (e.g., \"bert-base-uncased\" in the provided code).\n",
        "\n",
        "# In simpler terms:  Imagine you want to use a pre-trained model that understands English text.\n",
        "# This model likely uses a specific method to break down text into smaller pieces (tokens).\n",
        "# The `AutoTokenizer` figures out which method the model uses, and provides you with the right tool (the tokenizer) for converting your text into those pieces.\n",
        "\n",
        "# Example usage (as shown in the code):\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") #Loads the tokenizer for BERT\n",
        "text = \"I have a new unbreakable toy.\"\n",
        "tokens = tokenizer.tokenize(text) #Tokenizes the input text using the loaded tokenizer\n",
        "tokens\n",
        "\n",
        "#  The `tokenizer.tokenize()` method then splits the input text into tokens.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaE7Y6BahXqO",
        "outputId": "6d4c84c2-7670-40af-c0ef-b9f5ffb89dd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'have', 'a', 'new', 'un', '##break', '##able', 'toy', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}