{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9552db81-8050-4c39-9fee-3521bc789a53",
   "metadata": {},
   "source": [
    "# ğŸ“Œ NLP Text Preprocessing (Beginner-Friendly Interactive Guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b702ed-f832-45ad-861b-efd5432dc817",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Objective\n",
    "\n",
    "\n",
    "### Text preprocessing is an essential step in NLP. Before we apply deep learning algorithms, we must clean and format the text so that computers can process it effectively. This is to guide us for practical sessions of this course (text preprocessing), which includes:\n",
    "\n",
    "* Lowercasing\n",
    "* Removing Punctuation\n",
    "* Removing Stopwords\n",
    "* Stemming / Lemmatization\n",
    "\n",
    "### Before we can apply NLP techniques like sentiment analysis or topic modeling, we must clean and preprocess text data to:\n",
    "* âœ”ï¸ Ensure uniformity â†’ Convert text to lowercase to avoid treating \"Great\" and \"great\" differently.\n",
    "* âœ”ï¸ Remove unnecessary characters â†’ Punctuation, special symbols, and emojis.\n",
    "* âœ”ï¸ Eliminate stopwords â†’ Words that do not add significant meaning (e.g., \"the\", \"is\", \"and\").\n",
    "* âœ”ï¸ Apply stemming â†’ Reduce words to their base/root form (e.g., \"running\" â†’ \"run\").\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26588df7-a392-4524-8c81-dcc2d2923190",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Import Required Librariesatization\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e822a3-283a-4554-8d3f-cbb79e87f6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4ef289-dda6-4597-ba10-44174077eacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword Downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt Downloaded\n",
      "WordNet Downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK datasets\n",
    "nltk.download('stopwords')  # Stopwords list\n",
    "print(\"Stopword Downloaded\")\n",
    "nltk.download('punkt')  # Tokenization\n",
    "print(\"Punkt Downloaded\")\n",
    "nltk.download('wordnet')  # WordNet for Lemmatization\n",
    "print(\"WordNet Downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc18f6-b35f-40db-8325-6c871db81845",
   "metadata": {},
   "source": [
    "### ğŸ“Œ Why These Libraries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e983a619-deff-4fe4-ae96-501f0aa82a32",
   "metadata": {},
   "source": [
    "* pandas â†’ Stores structured text data in a table format.\n",
    "* re (regex) â†’ Removes punctuation and special characters.\n",
    "* nltk (Natural Language Toolkit) â†’ Handles stopwords, stemming, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60eae1-14d4-431b-8242-040e3cfefffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7960422c-ebdd-44cf-85c6-89b664daaa3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2ï¸âƒ£ Creating a Large, Diverse Dataset\n",
    "### We will create a dataset of 50 customer reviews with:\n",
    "* âœ”ï¸ Mixed sentiments (positive, negative, neutral)\n",
    "* âœ”ï¸ Punctuation and special characters\n",
    "* âœ”ï¸ Emojis for expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d05de11-4ec2-416e-903b-d0282b142b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset with 50 diverse reviews\n",
    "reviews = [\n",
    "    \"I absolutely LOVE this product!! â¤ï¸ It's super efficient and really worth the money. Definitely recommend! ğŸ‘\",\n",
    "    \"Worst purchase ever... ğŸ˜¡ Waste of money. DO NOT BUY!! Full of issues.\",\n",
    "    \"This product does what it says, but nothing special. ğŸ¤·â€â™‚ï¸ It's okay for the price, I guess.\",\n",
    "    \"AMAZING quality and fast shipping!!! ğŸš€ğŸ”¥ #satisfied #fastdelivery\",\n",
    "    \"Terrible! Had high expectations, but it broke in a week. Really disappointed. ğŸ˜\",\n",
    "    \"This phone is great ğŸ“±, but the battery drains too fast. ğŸ”‹ğŸ˜•\",\n",
    "    \"I love how easy it is to use! ğŸ¥° Definitely a game-changer.\",\n",
    "    \"Do not buy this laptop! ğŸ‘ It crashes every 10 minutes. So frustrating! ğŸ˜¡\",\n",
    "    \"The camera quality is excellent! ğŸ“¸ Love the night mode. ğŸŒ™âœ¨\",\n",
    "    \"Meh... the product is just average. ğŸ˜ I expected more for this price.\",\n",
    "    \"Great customer service! ğŸ™Œ They replaced my faulty item within 24 hours.\",\n",
    "    \"Horrible experience!! ğŸ’” Received a broken item and no refund.\",\n",
    "    \"I use this every day now. Super helpful! âœ…\",\n",
    "    \"The features are nice, but the software is laggy. ğŸ¤¦â€â™€ï¸ Annoying!\",\n",
    "    \"Best investment I've made this year! ğŸ”¥ğŸ”¥\",\n",
    "    \"Delivery took 2 months ğŸ˜¤ but the product is okay.\",\n",
    "    \"Wouldn't recommend to anyone. Waste of time. ğŸ™…â€â™‚ï¸\",\n",
    "    \"Super happy with my purchase!! ğŸ‰ Everything works perfectly.\",\n",
    "    \"This product changed my life. âœ¨ Absolutely incredible!\",\n",
    "    \"Not bad, but also not great. Just okay. ğŸ˜¶\",\n",
    "    \"Expected more for the price I paid. ğŸ˜•\",\n",
    "    \"Highly recommended! ğŸ‘ Fast shipping and great quality.\",\n",
    "    \"This is my third purchase from this store and I'm never disappointed! â¤ï¸\",\n",
    "    \"Overpriced and underwhelming. ğŸ«¤ Could be better.\",\n",
    "    \"Works well, but setup was a nightmare. ğŸ› ï¸ Took me 2 hours!\",\n",
    "    \"Love the design but the materials feel cheap. ğŸ§\",\n",
    "    \"Absolutely worth it!! ğŸ’ Super happy with this purchase.\",\n",
    "    \"Stopped working after a month. ğŸ˜” Very disappointed.\",\n",
    "    \"10/10! Would buy again. â­â­â­â­â­\",\n",
    "    \"Disgusting smell ğŸ¤¢, returned immediately.\",\n",
    "    \"Best headphones Iâ€™ve ever used! ğŸ§ Sound quality is top-notch.\",\n",
    "    \"Regret buying this. Not as advertised. ğŸ˜ \",\n",
    "    \"It does the job. Nothing exceptional. ğŸ¤·\",\n",
    "    \"The size is perfect, but the fabric feels cheap. ğŸ·ï¸\",\n",
    "    \"Feels premium! ğŸ” Great value for money.\",\n",
    "    \"Returned because it didn't fit. ğŸšš Hassle-free process.\",\n",
    "    \"The manual is useless. Had to figure it out myself. ğŸ“–\",\n",
    "    \"Why is this so expensive?? ğŸ’° Not worth the price.\",\n",
    "    \"Awesome customer support! ğŸˆ They solved my issue instantly.\",\n",
    "    \"Very fragile. Broke after one drop. ğŸ«£\",\n",
    "    \"Exactly what I needed! ğŸ¯ Highly recommended.\",\n",
    "    \"Didn't expect much, but it exceeded my expectations! ğŸŠ\",\n",
    "    \"Scratches easily, but functions well. ğŸ\",\n",
    "    \"This brand never disappoints! ğŸ† Will keep buying.\",\n",
    "    \"Fake reviews everywhere. Product is terrible. ğŸ˜’\",\n",
    "    \"No wordsâ€¦ just amazing. ğŸ”¥ğŸ”¥ğŸ”¥\",\n",
    "    \"This was a gift and the recipient loved it! ğŸ\",\n",
    "    \"Cheap plastic, feels like a toy. ğŸ˜¡ Not recommended.\",\n",
    "    \"Perfect for my needs. ğŸ‘ Would purchase again.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329d0568-d74f-420e-a345-804456372fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I absolutely LOVE this product!! â¤ï¸ It's super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst purchase ever... ğŸ˜¡ Waste of money. DO NO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This product does what it says, but nothing sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMAZING quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Terrible! Had high expectations, but it broke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This phone is great ğŸ“±, but the battery drains ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love how easy it is to use! ğŸ¥° Definitely a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Do not buy this laptop! ğŸ‘ It crashes every 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The camera quality is excellent! ğŸ“¸ Love the ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Meh... the product is just average. ğŸ˜ I expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Great customer service! ğŸ™Œ They replaced my fau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Horrible experience!! ğŸ’” Received a broken item...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I use this every day now. Super helpful! âœ…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The features are nice, but the software is lag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Best investment I've made this year! ğŸ”¥ğŸ”¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Delivery took 2 months ğŸ˜¤ but the product is okay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wouldn't recommend to anyone. Waste of time. ğŸ™…â€â™‚ï¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Super happy with my purchase!! ğŸ‰ Everything wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This product changed my life. âœ¨ Absolutely inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Not bad, but also not great. Just okay. ğŸ˜¶</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review\n",
       "0   I absolutely LOVE this product!! â¤ï¸ It's super...\n",
       "1   Worst purchase ever... ğŸ˜¡ Waste of money. DO NO...\n",
       "2   This product does what it says, but nothing sp...\n",
       "3   AMAZING quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...\n",
       "4   Terrible! Had high expectations, but it broke ...\n",
       "5   This phone is great ğŸ“±, but the battery drains ...\n",
       "6   I love how easy it is to use! ğŸ¥° Definitely a g...\n",
       "7   Do not buy this laptop! ğŸ‘ It crashes every 10 ...\n",
       "8   The camera quality is excellent! ğŸ“¸ Love the ni...\n",
       "9   Meh... the product is just average. ğŸ˜ I expect...\n",
       "10  Great customer service! ğŸ™Œ They replaced my fau...\n",
       "11  Horrible experience!! ğŸ’” Received a broken item...\n",
       "12         I use this every day now. Super helpful! âœ…\n",
       "13  The features are nice, but the software is lag...\n",
       "14            Best investment I've made this year! ğŸ”¥ğŸ”¥\n",
       "15  Delivery took 2 months ğŸ˜¤ but the product is okay.\n",
       "16  Wouldn't recommend to anyone. Waste of time. ğŸ™…â€â™‚ï¸\n",
       "17  Super happy with my purchase!! ğŸ‰ Everything wo...\n",
       "18  This product changed my life. âœ¨ Absolutely inc...\n",
       "19          Not bad, but also not great. Just okay. ğŸ˜¶"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "df_reviews = pd.DataFrame(reviews, columns=[\"Review\"])\n",
    "\n",
    "# Display the dataset\n",
    "df_reviews.head(20)  # Show first 10 rows for preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca9c5ad-7d32-46b3-8f2f-9e9fc814dc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e69349f-53ff-4d89-830a-abe4f031160a",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Lowercasing\n",
    "### ğŸ“Œ Why? \n",
    "### Because NLP models treat \"Love\" and \"love\" differently, so we standardize text by converting it all to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d9a752-895e-45d7-8b12-fd0e7bbc2887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Lowercase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I absolutely LOVE this product!! â¤ï¸ It's super...</td>\n",
       "      <td>i absolutely love this product!! â¤ï¸ it's super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst purchase ever... ğŸ˜¡ Waste of money. DO NO...</td>\n",
       "      <td>worst purchase ever... ğŸ˜¡ waste of money. do no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This product does what it says, but nothing sp...</td>\n",
       "      <td>this product does what it says, but nothing sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMAZING quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...</td>\n",
       "      <td>amazing quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Terrible! Had high expectations, but it broke ...</td>\n",
       "      <td>terrible! had high expectations, but it broke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This phone is great ğŸ“±, but the battery drains ...</td>\n",
       "      <td>this phone is great ğŸ“±, but the battery drains ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love how easy it is to use! ğŸ¥° Definitely a g...</td>\n",
       "      <td>i love how easy it is to use! ğŸ¥° definitely a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Do not buy this laptop! ğŸ‘ It crashes every 10 ...</td>\n",
       "      <td>do not buy this laptop! ğŸ‘ it crashes every 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The camera quality is excellent! ğŸ“¸ Love the ni...</td>\n",
       "      <td>the camera quality is excellent! ğŸ“¸ love the ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Meh... the product is just average. ğŸ˜ I expect...</td>\n",
       "      <td>meh... the product is just average. ğŸ˜ i expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Great customer service! ğŸ™Œ They replaced my fau...</td>\n",
       "      <td>great customer service! ğŸ™Œ they replaced my fau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Horrible experience!! ğŸ’” Received a broken item...</td>\n",
       "      <td>horrible experience!! ğŸ’” received a broken item...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I use this every day now. Super helpful! âœ…</td>\n",
       "      <td>i use this every day now. super helpful! âœ…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The features are nice, but the software is lag...</td>\n",
       "      <td>the features are nice, but the software is lag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Best investment I've made this year! ğŸ”¥ğŸ”¥</td>\n",
       "      <td>best investment i've made this year! ğŸ”¥ğŸ”¥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Delivery took 2 months ğŸ˜¤ but the product is okay.</td>\n",
       "      <td>delivery took 2 months ğŸ˜¤ but the product is okay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wouldn't recommend to anyone. Waste of time. ğŸ™…â€â™‚ï¸</td>\n",
       "      <td>wouldn't recommend to anyone. waste of time. ğŸ™…â€â™‚ï¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Super happy with my purchase!! ğŸ‰ Everything wo...</td>\n",
       "      <td>super happy with my purchase!! ğŸ‰ everything wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This product changed my life. âœ¨ Absolutely inc...</td>\n",
       "      <td>this product changed my life. âœ¨ absolutely inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Not bad, but also not great. Just okay. ğŸ˜¶</td>\n",
       "      <td>not bad, but also not great. just okay. ğŸ˜¶</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  \\\n",
       "0   I absolutely LOVE this product!! â¤ï¸ It's super...   \n",
       "1   Worst purchase ever... ğŸ˜¡ Waste of money. DO NO...   \n",
       "2   This product does what it says, but nothing sp...   \n",
       "3   AMAZING quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...   \n",
       "4   Terrible! Had high expectations, but it broke ...   \n",
       "5   This phone is great ğŸ“±, but the battery drains ...   \n",
       "6   I love how easy it is to use! ğŸ¥° Definitely a g...   \n",
       "7   Do not buy this laptop! ğŸ‘ It crashes every 10 ...   \n",
       "8   The camera quality is excellent! ğŸ“¸ Love the ni...   \n",
       "9   Meh... the product is just average. ğŸ˜ I expect...   \n",
       "10  Great customer service! ğŸ™Œ They replaced my fau...   \n",
       "11  Horrible experience!! ğŸ’” Received a broken item...   \n",
       "12         I use this every day now. Super helpful! âœ…   \n",
       "13  The features are nice, but the software is lag...   \n",
       "14            Best investment I've made this year! ğŸ”¥ğŸ”¥   \n",
       "15  Delivery took 2 months ğŸ˜¤ but the product is okay.   \n",
       "16  Wouldn't recommend to anyone. Waste of time. ğŸ™…â€â™‚ï¸   \n",
       "17  Super happy with my purchase!! ğŸ‰ Everything wo...   \n",
       "18  This product changed my life. âœ¨ Absolutely inc...   \n",
       "19          Not bad, but also not great. Just okay. ğŸ˜¶   \n",
       "\n",
       "                                     Review_Lowercase  \n",
       "0   i absolutely love this product!! â¤ï¸ it's super...  \n",
       "1   worst purchase ever... ğŸ˜¡ waste of money. do no...  \n",
       "2   this product does what it says, but nothing sp...  \n",
       "3   amazing quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...  \n",
       "4   terrible! had high expectations, but it broke ...  \n",
       "5   this phone is great ğŸ“±, but the battery drains ...  \n",
       "6   i love how easy it is to use! ğŸ¥° definitely a g...  \n",
       "7   do not buy this laptop! ğŸ‘ it crashes every 10 ...  \n",
       "8   the camera quality is excellent! ğŸ“¸ love the ni...  \n",
       "9   meh... the product is just average. ğŸ˜ i expect...  \n",
       "10  great customer service! ğŸ™Œ they replaced my fau...  \n",
       "11  horrible experience!! ğŸ’” received a broken item...  \n",
       "12         i use this every day now. super helpful! âœ…  \n",
       "13  the features are nice, but the software is lag...  \n",
       "14            best investment i've made this year! ğŸ”¥ğŸ”¥  \n",
       "15  delivery took 2 months ğŸ˜¤ but the product is okay.  \n",
       "16  wouldn't recommend to anyone. waste of time. ğŸ™…â€â™‚ï¸  \n",
       "17  super happy with my purchase!! ğŸ‰ everything wo...  \n",
       "18  this product changed my life. âœ¨ absolutely inc...  \n",
       "19          not bad, but also not great. just okay. ğŸ˜¶  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert text to lowercase\n",
    "df_reviews[\"Review_Lowercase\"] = df_reviews[\"Review\"].str.lower()\n",
    "\n",
    "# Display results\n",
    "df_reviews.head(20)  # Show first 10 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f84e5-a0f0-4372-8879-6ba19676d7cc",
   "metadata": {},
   "source": [
    "### âœ… Effect:\n",
    "\n",
    "#### \"LOVE\" â†’ \"love\"\n",
    "#### \"BEST\" â†’ \"best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a73c48-bc4c-41b0-a082-c29beb724b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47f8f959-43ba-4664-b8f6-96c6d9a365f8",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Removing Punctuation Marks\n",
    "### ğŸ“Œ Why?\n",
    "### Punctuation doesn't contribute meaning to most NLP models.\n",
    "### Removing punctuation simplifies text without affecting readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680adea5-8a28-4c38-b6eb-51bdbf2faf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Lowercase</th>\n",
       "      <th>Review_NoPunct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I absolutely LOVE this product!! â¤ï¸ It's super...</td>\n",
       "      <td>i absolutely love this product!! â¤ï¸ it's super...</td>\n",
       "      <td>i absolutely love this product  its super effi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst purchase ever... ğŸ˜¡ Waste of money. DO NO...</td>\n",
       "      <td>worst purchase ever... ğŸ˜¡ waste of money. do no...</td>\n",
       "      <td>worst purchase ever  waste of money do not buy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This product does what it says, but nothing sp...</td>\n",
       "      <td>this product does what it says, but nothing sp...</td>\n",
       "      <td>this product does what it says but nothing spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMAZING quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...</td>\n",
       "      <td>amazing quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...</td>\n",
       "      <td>amazing quality and fast shipping  satisfied f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Terrible! Had high expectations, but it broke ...</td>\n",
       "      <td>terrible! had high expectations, but it broke ...</td>\n",
       "      <td>terrible had high expectations but it broke in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This phone is great ğŸ“±, but the battery drains ...</td>\n",
       "      <td>this phone is great ğŸ“±, but the battery drains ...</td>\n",
       "      <td>this phone is great  but the battery drains to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love how easy it is to use! ğŸ¥° Definitely a g...</td>\n",
       "      <td>i love how easy it is to use! ğŸ¥° definitely a g...</td>\n",
       "      <td>i love how easy it is to use  definitely a gam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Do not buy this laptop! ğŸ‘ It crashes every 10 ...</td>\n",
       "      <td>do not buy this laptop! ğŸ‘ it crashes every 10 ...</td>\n",
       "      <td>do not buy this laptop  it crashes every 10 mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The camera quality is excellent! ğŸ“¸ Love the ni...</td>\n",
       "      <td>the camera quality is excellent! ğŸ“¸ love the ni...</td>\n",
       "      <td>the camera quality is excellent  love the nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Meh... the product is just average. ğŸ˜ I expect...</td>\n",
       "      <td>meh... the product is just average. ğŸ˜ i expect...</td>\n",
       "      <td>meh the product is just average  i expected mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Great customer service! ğŸ™Œ They replaced my fau...</td>\n",
       "      <td>great customer service! ğŸ™Œ they replaced my fau...</td>\n",
       "      <td>great customer service  they replaced my fault...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Horrible experience!! ğŸ’” Received a broken item...</td>\n",
       "      <td>horrible experience!! ğŸ’” received a broken item...</td>\n",
       "      <td>horrible experience  received a broken item an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I use this every day now. Super helpful! âœ…</td>\n",
       "      <td>i use this every day now. super helpful! âœ…</td>\n",
       "      <td>i use this every day now super helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The features are nice, but the software is lag...</td>\n",
       "      <td>the features are nice, but the software is lag...</td>\n",
       "      <td>the features are nice but the software is lagg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Best investment I've made this year! ğŸ”¥ğŸ”¥</td>\n",
       "      <td>best investment i've made this year! ğŸ”¥ğŸ”¥</td>\n",
       "      <td>best investment ive made this year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Delivery took 2 months ğŸ˜¤ but the product is okay.</td>\n",
       "      <td>delivery took 2 months ğŸ˜¤ but the product is okay.</td>\n",
       "      <td>delivery took 2 months  but the product is okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wouldn't recommend to anyone. Waste of time. ğŸ™…â€â™‚ï¸</td>\n",
       "      <td>wouldn't recommend to anyone. waste of time. ğŸ™…â€â™‚ï¸</td>\n",
       "      <td>wouldnt recommend to anyone waste of time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Super happy with my purchase!! ğŸ‰ Everything wo...</td>\n",
       "      <td>super happy with my purchase!! ğŸ‰ everything wo...</td>\n",
       "      <td>super happy with my purchase  everything works...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This product changed my life. âœ¨ Absolutely inc...</td>\n",
       "      <td>this product changed my life. âœ¨ absolutely inc...</td>\n",
       "      <td>this product changed my life  absolutely incre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Not bad, but also not great. Just okay. ğŸ˜¶</td>\n",
       "      <td>not bad, but also not great. just okay. ğŸ˜¶</td>\n",
       "      <td>not bad but also not great just okay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  \\\n",
       "0   I absolutely LOVE this product!! â¤ï¸ It's super...   \n",
       "1   Worst purchase ever... ğŸ˜¡ Waste of money. DO NO...   \n",
       "2   This product does what it says, but nothing sp...   \n",
       "3   AMAZING quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...   \n",
       "4   Terrible! Had high expectations, but it broke ...   \n",
       "5   This phone is great ğŸ“±, but the battery drains ...   \n",
       "6   I love how easy it is to use! ğŸ¥° Definitely a g...   \n",
       "7   Do not buy this laptop! ğŸ‘ It crashes every 10 ...   \n",
       "8   The camera quality is excellent! ğŸ“¸ Love the ni...   \n",
       "9   Meh... the product is just average. ğŸ˜ I expect...   \n",
       "10  Great customer service! ğŸ™Œ They replaced my fau...   \n",
       "11  Horrible experience!! ğŸ’” Received a broken item...   \n",
       "12         I use this every day now. Super helpful! âœ…   \n",
       "13  The features are nice, but the software is lag...   \n",
       "14            Best investment I've made this year! ğŸ”¥ğŸ”¥   \n",
       "15  Delivery took 2 months ğŸ˜¤ but the product is okay.   \n",
       "16  Wouldn't recommend to anyone. Waste of time. ğŸ™…â€â™‚ï¸   \n",
       "17  Super happy with my purchase!! ğŸ‰ Everything wo...   \n",
       "18  This product changed my life. âœ¨ Absolutely inc...   \n",
       "19          Not bad, but also not great. Just okay. ğŸ˜¶   \n",
       "\n",
       "                                     Review_Lowercase  \\\n",
       "0   i absolutely love this product!! â¤ï¸ it's super...   \n",
       "1   worst purchase ever... ğŸ˜¡ waste of money. do no...   \n",
       "2   this product does what it says, but nothing sp...   \n",
       "3   amazing quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...   \n",
       "4   terrible! had high expectations, but it broke ...   \n",
       "5   this phone is great ğŸ“±, but the battery drains ...   \n",
       "6   i love how easy it is to use! ğŸ¥° definitely a g...   \n",
       "7   do not buy this laptop! ğŸ‘ it crashes every 10 ...   \n",
       "8   the camera quality is excellent! ğŸ“¸ love the ni...   \n",
       "9   meh... the product is just average. ğŸ˜ i expect...   \n",
       "10  great customer service! ğŸ™Œ they replaced my fau...   \n",
       "11  horrible experience!! ğŸ’” received a broken item...   \n",
       "12         i use this every day now. super helpful! âœ…   \n",
       "13  the features are nice, but the software is lag...   \n",
       "14            best investment i've made this year! ğŸ”¥ğŸ”¥   \n",
       "15  delivery took 2 months ğŸ˜¤ but the product is okay.   \n",
       "16  wouldn't recommend to anyone. waste of time. ğŸ™…â€â™‚ï¸   \n",
       "17  super happy with my purchase!! ğŸ‰ everything wo...   \n",
       "18  this product changed my life. âœ¨ absolutely inc...   \n",
       "19          not bad, but also not great. just okay. ğŸ˜¶   \n",
       "\n",
       "                                       Review_NoPunct  \n",
       "0   i absolutely love this product  its super effi...  \n",
       "1   worst purchase ever  waste of money do not buy...  \n",
       "2   this product does what it says but nothing spe...  \n",
       "3   amazing quality and fast shipping  satisfied f...  \n",
       "4   terrible had high expectations but it broke in...  \n",
       "5   this phone is great  but the battery drains to...  \n",
       "6   i love how easy it is to use  definitely a gam...  \n",
       "7   do not buy this laptop  it crashes every 10 mi...  \n",
       "8   the camera quality is excellent  love the nigh...  \n",
       "9   meh the product is just average  i expected mo...  \n",
       "10  great customer service  they replaced my fault...  \n",
       "11  horrible experience  received a broken item an...  \n",
       "12            i use this every day now super helpful   \n",
       "13  the features are nice but the software is lagg...  \n",
       "14                best investment ive made this year   \n",
       "15    delivery took 2 months  but the product is okay  \n",
       "16         wouldnt recommend to anyone waste of time   \n",
       "17  super happy with my purchase  everything works...  \n",
       "18  this product changed my life  absolutely incre...  \n",
       "19              not bad but also not great just okay   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to remove punctuation from text\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    This function removes all punctuation marks from the given text.\n",
    "    \n",
    "    - The function uses `re.sub(r'[^\\w\\s]', '', text)`, which means:\n",
    "      - `[^\\w\\s]` â†’ Matches any character that is NOT a word (`\\w`) or a whitespace (`\\s`).\n",
    "      - `''` â†’ Replaces all matched punctuation with an empty string.\n",
    "    \n",
    "    Example:\n",
    "    ----------\n",
    "    Input  : \"Hello, World!!!\"\n",
    "    Output : \"Hello World\"\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Apply the function to each review in the dataset\n",
    "df_reviews[\"Review_NoPunct\"] = df_reviews[\"Review_Lowercase\"].apply(remove_punctuation)\n",
    "\n",
    "# Display the first 20 rows to observe the changes\n",
    "df_reviews.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc73085-9080-4e71-a0a5-8382d3d021cb",
   "metadata": {},
   "source": [
    "### âœ… Effect:\n",
    "\n",
    "#### \"Amazing product!!!\" â†’ \"amazing product\"\n",
    "#### \"Good, but pricey.\" â†’ \"good but pricey\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55375c7-d41c-4f63-987f-c1f31ce23530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e8cb1c6-daf6-443f-a39d-92fa9449171d",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Removing Stopwords\n",
    "### ğŸ“Œ Why?\n",
    "### Stopwords (e.g., \"is\", \"the\", \"of\") appear frequently but add little meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be55b03c-2cab-4c47-9a4b-b473c5163c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Lowercase</th>\n",
       "      <th>Review_NoPunct</th>\n",
       "      <th>Review_NoStopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I absolutely LOVE this product!! â¤ï¸ It's super...</td>\n",
       "      <td>i absolutely love this product!! â¤ï¸ it's super...</td>\n",
       "      <td>i absolutely love this product  its super effi...</td>\n",
       "      <td>absolutely love product super efficient really...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worst purchase ever... ğŸ˜¡ Waste of money. DO NO...</td>\n",
       "      <td>worst purchase ever... ğŸ˜¡ waste of money. do no...</td>\n",
       "      <td>worst purchase ever  waste of money do not buy...</td>\n",
       "      <td>worst purchase ever waste money buy full issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This product does what it says, but nothing sp...</td>\n",
       "      <td>this product does what it says, but nothing sp...</td>\n",
       "      <td>this product does what it says but nothing spe...</td>\n",
       "      <td>product says nothing special okay price guess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMAZING quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...</td>\n",
       "      <td>amazing quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...</td>\n",
       "      <td>amazing quality and fast shipping  satisfied f...</td>\n",
       "      <td>amazing quality fast shipping satisfied fastde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Terrible! Had high expectations, but it broke ...</td>\n",
       "      <td>terrible! had high expectations, but it broke ...</td>\n",
       "      <td>terrible had high expectations but it broke in...</td>\n",
       "      <td>terrible high expectations broke week really d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This phone is great ğŸ“±, but the battery drains ...</td>\n",
       "      <td>this phone is great ğŸ“±, but the battery drains ...</td>\n",
       "      <td>this phone is great  but the battery drains to...</td>\n",
       "      <td>phone great battery drains fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love how easy it is to use! ğŸ¥° Definitely a g...</td>\n",
       "      <td>i love how easy it is to use! ğŸ¥° definitely a g...</td>\n",
       "      <td>i love how easy it is to use  definitely a gam...</td>\n",
       "      <td>love easy use definitely gamechanger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Do not buy this laptop! ğŸ‘ It crashes every 10 ...</td>\n",
       "      <td>do not buy this laptop! ğŸ‘ it crashes every 10 ...</td>\n",
       "      <td>do not buy this laptop  it crashes every 10 mi...</td>\n",
       "      <td>buy laptop crashes every 10 minutes frustrating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The camera quality is excellent! ğŸ“¸ Love the ni...</td>\n",
       "      <td>the camera quality is excellent! ğŸ“¸ love the ni...</td>\n",
       "      <td>the camera quality is excellent  love the nigh...</td>\n",
       "      <td>camera quality excellent love night mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Meh... the product is just average. ğŸ˜ I expect...</td>\n",
       "      <td>meh... the product is just average. ğŸ˜ i expect...</td>\n",
       "      <td>meh the product is just average  i expected mo...</td>\n",
       "      <td>meh product average expected price</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  \\\n",
       "0  I absolutely LOVE this product!! â¤ï¸ It's super...   \n",
       "1  Worst purchase ever... ğŸ˜¡ Waste of money. DO NO...   \n",
       "2  This product does what it says, but nothing sp...   \n",
       "3  AMAZING quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...   \n",
       "4  Terrible! Had high expectations, but it broke ...   \n",
       "5  This phone is great ğŸ“±, but the battery drains ...   \n",
       "6  I love how easy it is to use! ğŸ¥° Definitely a g...   \n",
       "7  Do not buy this laptop! ğŸ‘ It crashes every 10 ...   \n",
       "8  The camera quality is excellent! ğŸ“¸ Love the ni...   \n",
       "9  Meh... the product is just average. ğŸ˜ I expect...   \n",
       "\n",
       "                                    Review_Lowercase  \\\n",
       "0  i absolutely love this product!! â¤ï¸ it's super...   \n",
       "1  worst purchase ever... ğŸ˜¡ waste of money. do no...   \n",
       "2  this product does what it says, but nothing sp...   \n",
       "3  amazing quality and fast shipping!!! ğŸš€ğŸ”¥ #satis...   \n",
       "4  terrible! had high expectations, but it broke ...   \n",
       "5  this phone is great ğŸ“±, but the battery drains ...   \n",
       "6  i love how easy it is to use! ğŸ¥° definitely a g...   \n",
       "7  do not buy this laptop! ğŸ‘ it crashes every 10 ...   \n",
       "8  the camera quality is excellent! ğŸ“¸ love the ni...   \n",
       "9  meh... the product is just average. ğŸ˜ i expect...   \n",
       "\n",
       "                                      Review_NoPunct  \\\n",
       "0  i absolutely love this product  its super effi...   \n",
       "1  worst purchase ever  waste of money do not buy...   \n",
       "2  this product does what it says but nothing spe...   \n",
       "3  amazing quality and fast shipping  satisfied f...   \n",
       "4  terrible had high expectations but it broke in...   \n",
       "5  this phone is great  but the battery drains to...   \n",
       "6  i love how easy it is to use  definitely a gam...   \n",
       "7  do not buy this laptop  it crashes every 10 mi...   \n",
       "8  the camera quality is excellent  love the nigh...   \n",
       "9  meh the product is just average  i expected mo...   \n",
       "\n",
       "                                  Review_NoStopwords  \n",
       "0  absolutely love product super efficient really...  \n",
       "1    worst purchase ever waste money buy full issues  \n",
       "2      product says nothing special okay price guess  \n",
       "3  amazing quality fast shipping satisfied fastde...  \n",
       "4  terrible high expectations broke week really d...  \n",
       "5                    phone great battery drains fast  \n",
       "6               love easy use definitely gamechanger  \n",
       "7    buy laptop crashes every 10 minutes frustrating  \n",
       "8           camera quality excellent love night mode  \n",
       "9                 meh product average expected price  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load stopwords from the NLTK library\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "\"\"\"\n",
    "Explanation:\n",
    "------------\n",
    "- `stopwords.words('english')` loads a predefined list of common English stopwords.\n",
    "- `set(stopwords.words('english'))` converts the list into a set for **faster lookup**.\n",
    "- Example stopwords: {\"the\", \"is\", \"in\", \"at\", \"which\", \"and\", \"but\", \"or\", \"a\", \"an\"}\n",
    "\n",
    "Why use a set?\n",
    "--------------\n",
    "- Checking if a word is in a **set** is faster (O(1) time complexity) compared to a **list** (O(n)).\n",
    "- This makes the stopword removal process much **more efficient** for large datasets.\n",
    "\"\"\"\n",
    "\n",
    "# Function to remove stopwords from a given text\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    This function removes all stopwords from the given text.\n",
    "    \n",
    "    - It takes a sentence, splits it into individual words, \n",
    "      and filters out any word that is present in the `stop_words` set.\n",
    "    - It then joins the remaining words back into a single cleaned string.\n",
    "\n",
    "    Example:\n",
    "    ----------\n",
    "    Input  : \"this is a great product with amazing quality\"\n",
    "    Output : \"great product amazing quality\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the sentence into words and keep only words not in stop_words\n",
    "    cleaned_text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    \n",
    "    return cleaned_text  # Return the processed text without stopwords\n",
    "\n",
    "# Apply the remove_stopwords function to each review in the dataset\n",
    "df_reviews[\"Review_NoStopwords\"] = df_reviews[\"Review_NoPunct\"].apply(remove_stopwords)\n",
    "\n",
    "# Display the first 10 rows to observe the changes\n",
    "df_reviews.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cae56-b82c-44f5-be46-379799ed2d05",
   "metadata": {},
   "source": [
    "### âœ… Effect:\n",
    "\n",
    "#### \"this product is very good\" â†’ \"product good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be795e-a81e-4042-a457-199385e35a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "945052d1-03ee-4f67-a0eb-45629a9a4db2",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Stemming & Lemmatization (Reducing Words to Their Base Form)\n",
    "### ğŸ“Œ Why is this step important?\n",
    "### Both stemming and lemmatization reduce words to their base form, but they work differently.\n",
    "### This is crucial for NLP models because different word forms (e.g., \"running\", \"ran\", \"runs\") should be treated as the same word.\n",
    "### Example:\n",
    "### Without processing: \"He is running and she ran fast\"\n",
    "### With stemming: \"He is run and she ran fast\"\n",
    "### With lemmatization: \"He be running and she run fast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e80731aa-2c88-4997-bd42-2dad88cec610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['run', 'fli', 'happi', 'better', 'wolv', 'studi']\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStemmed Words:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stemmed_words)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Apply Lemmatization\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m lemmatized_words \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLemmatized Words:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lemmatized_words)\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStemmed Words:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stemmed_words)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Apply Lemmatization\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m lemmatized_words \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLemmatized Words:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lemmatized_words)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download WordNet dataset for lemmatization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Stemmer and Lemmatizer\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Test words\n",
    "words = [\"running\", \"flies\", \"happiness\", \"better\", \"wolves\", \"studies\"]\n",
    "\n",
    "# Apply Stemming\n",
    "stemmed_words = [ps.stem(word) for word in words]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "\n",
    "# Apply Lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036bd47-703a-4228-9a32-ad24b6c26e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Porter Stemmer (for stemming) and WordNet Lemmatizer (for lemmatization)\n",
    "ps = PorterStemmer()  # Stemming\n",
    "lemmatizer = WordNetLemmatizer()  # Lemmatization\n",
    "\n",
    "\"\"\"\n",
    "What is Stemming?\n",
    "-----------------\n",
    "- Stemming chops words down to their root form by **removing suffixes**.\n",
    "- It applies **heuristic rules** rather than a linguistic approach.\n",
    "- It does NOT always produce a real English word.\n",
    "\n",
    "Example:\n",
    "- \"running\" â†’ \"run\"\n",
    "- \"happiness\" â†’ \"happi\"\n",
    "- \"better\" â†’ \"better\"  (incorrect, should be \"good\")\n",
    "\n",
    "What is Lemmatization?\n",
    "----------------------\n",
    "- Lemmatization is **more accurate** than stemming.\n",
    "- It uses a **dictionary-based approach** to return the correct base form of a word.\n",
    "- It ensures words are **real** words in the English language.\n",
    "\n",
    "Example:\n",
    "- \"running\" â†’ \"run\"\n",
    "- \"happiness\" â†’ \"happiness\"  (unchanged because itâ€™s already a base form)\n",
    "- \"better\" â†’ \"good\"  (correctly mapped)\n",
    "\"\"\"\n",
    "\n",
    "# Function to apply stemming\n",
    "def apply_stemming(text):\n",
    "    \"\"\"\n",
    "    This function applies stemming to all words in a given text.\n",
    "    \n",
    "    - It splits the sentence into individual words.\n",
    "    - Each word is reduced to its base form using the Porter Stemmer.\n",
    "    - The words are then joined back into a processed string.\n",
    "\n",
    "    Example:\n",
    "    ----------\n",
    "    Input  : \"running quickly towards happiness\"\n",
    "    Output : \"run quickli toward happi\"\n",
    "    \"\"\"\n",
    "    return ' '.join(ps.stem(word) for word in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6efa994-9588-4744-bf3b-52dd11808251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to the reviews\n",
    "df_reviews[\"Review_Stemmed\"] = df_reviews[\"Review_NoStopwords\"].apply(apply_stemming)\n",
    "\n",
    "# Display the first 10 rows to observe changes\n",
    "df_reviews.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a94eeb-8b4b-48bd-8480-0b112a6dadc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Download additional WordNet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72eb6b75-2692-482d-a5f2-127b4341c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81708a2a-0690-4d05-ae92-4a76c6cc4229",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:/Users/DELL/AppData/Roaming/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:/Users/DELL/AppData/Roaming/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m      4\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrocks :\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorpora :\u001b[39m\u001b[38;5;124m\"\u001b[39m, lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorpora\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# a denotes adjective in \"pos\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DELL/nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DELL\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:/Users/DELL/AppData/Roaming/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos=\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1e76e84-abb2-4496-ba20-28af6e20c6c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load English model (after installation)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspacy_lemmatization\u001b[39m(text):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    Applies SpaCy lemmatization to a given text.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    - Uses the \"en_core_web_sm\" model to analyze words.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    - Converts each word to its base form (lemma).\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model (after installation)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_lemmatization(text):\n",
    "    \"\"\"\n",
    "    Applies SpaCy lemmatization to a given text.\n",
    "    \n",
    "    - Uses the \"en_core_web_sm\" model to analyze words.\n",
    "    - Converts each word to its base form (lemma).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "# Example usage\n",
    "print(spacy_lemmatization(\"wolves better going\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35f83d7-1f2c-455d-8a56-39183a072a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"C:/Users/DELL/AppData/Roaming/nltk_data\")  # Change this path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40e38570-7378-443e-a8dd-b56693f17f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet', download_dir=\"C:/Users/DELL/AppData/Roaming/nltk_data\")\n",
    "nltk.download('omw-1.4', download_dir=\"C:/Users/DELL/AppData/Roaming/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c774f-d43a-434b-94e4-9a87d503d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure that WordNet is downloaded\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to apply lemmatization\n",
    "def apply_lemmatization(text):\n",
    "    \"\"\"\n",
    "    This function applies lemmatization to all words in a given text.\n",
    "    \n",
    "    - It splits the sentence into individual words.\n",
    "    - Each word is converted to its base form using the WordNet Lemmatizer.\n",
    "    - The words are then joined back into a processed string.\n",
    "\n",
    "    Example:\n",
    "    ----------\n",
    "    Input  : \"wolves better going\"\n",
    "    Output : \"wolf good going\"  (correct transformations)\n",
    "    \"\"\"\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "\n",
    "# Apply lemmatization to the reviews\n",
    "df_reviews[\"Review_Lemmatized\"] = df_reviews[\"Review_NoStopwords\"].apply(apply_lemmatization)\n",
    "\n",
    "# Display the first 10 rows to observe changes\n",
    "df_reviews.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832552f-4740-4a4c-abd4-fae29fcb9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply lemmatization\n",
    "def apply_lemmatization(text):\n",
    "    \"\"\"\n",
    "    This function applies lemmatization to all words in a given text.\n",
    "    \n",
    "    - It splits the sentence into individual words.\n",
    "    - Each word is converted to its base form using the WordNet Lemmatizer.\n",
    "    - The words are then joined back into a processed string.\n",
    "\n",
    "    Example:\n",
    "    ----------\n",
    "    Input  : \"running quickly towards happiness\"\n",
    "    Output : \"running quickly towards happiness\"  (unchanged)\n",
    "    \n",
    "    Input  : \"wolves better going\"\n",
    "    Output : \"wolf good going\"  (correct transformations)\n",
    "    \"\"\"\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1926cf0-59bc-4235-acd2-aa0786227de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to the reviews\n",
    "df_reviews[\"Review_Lemmatized\"] = df_reviews[\"Review_NoStopwords\"].apply(apply_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5bc360-aecb-4e8c-b194-db508a689b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
